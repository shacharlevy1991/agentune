from __future__ import annotations

import datetime
import heapq
import logging
import math
from abc import abstractmethod
from collections.abc import Sequence
from typing import cast, override

import numpy as np
import polars as pl
from attrs import define, field
from scipy.stats import spearmanr
from sklearn.feature_selection import mutual_info_classif

from .base import (
    EfficientEvaluator,
    EfficientEvaluatorMetric,
    EfficientEvaluatorParams,
    EfficientEvaluatorProgressCallback,
    EfficientEvaluatorResult,
    FeatureInputs,
    FeatureVariant,
    FeatureVariantEvalState,
)

_logger = logging.getLogger(__name__)

class MetricWithUncertainty(EfficientEvaluatorMetric):
    @property
    @abstractmethod
    def _min_samples_for_max_error(self) -> pl.DataFrame: ...
    
    def uncertainty(self, samples: int) -> float:
        max_error = cast(float | None, self._min_samples_for_max_error.filter(pl.col('min_sample_size') <= samples) \
            ['max_subsequent_error'].min())
        if max_error is None:
            uncertainty = 1.0 # Max uncertainty
        else:
            # Max error is in %, uncertainty is in [0, 1]
            uncertainty = 1 - ((100-max_error) / 100.0)
        return uncertainty

class RegressionCorrelationMetric(MetricWithUncertainty):

    # Empirical constants generated by tests/feature/eval/incremental_metrics.py
    # These hold if the underlying correlation we're sampling from is >= 0.05
    _min_samples_for_max_error: pl.DataFrame = pl.DataFrame(
        {'max_subsequent_error': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 9.0, 11.0, 12.0], 
         'min_sample_size': [2500, 840, 690, 180, 140, 120, 90, 50, 40, 30]})

    def calculate(self, outputs: pl.Series, target: pl.Series) -> tuple[float, float]:
        corr: float
        corr, _ = spearmanr(outputs.to_numpy(), target.to_numpy()) # type: ignore[assignment]
        return (corr, self.uncertainty(len(outputs)))
   
class ClassificationRigMetric(MetricWithUncertainty):

    # Empirical constants generated by tests/feature/eval/incremental_metrics.py
    # These hold if the underlying RIG is ~~ >= 0.028 (a correlation of ~ 0.2 for binary classification)
    # (For n_classes > 2 and the same correlation, the RIG would be much higher)
    _min_samples_for_max_error: pl.DataFrame = pl.DataFrame(
        {'max_subsequent_error': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 9.0, 10.0, 12.0, 14.0], 
         'min_sample_size': [3100, 1180, 790, 410, 350, 230, 130, 100, 80, 70, 40]})
        
    def _shannon_entropy(self, target: np.ndarray) -> float:
        """Calculate Shannon entropy for discrete/categorical data.
        H(Y) = -Î£ p(y) * log(p(y))
        """
        _, counts = np.unique(target, return_counts=True)
        probabilities = counts / len(target)

        # Calculate Shannon entropy (using natural log to match MI calculations)
        entropy = -np.sum(probabilities * np.log(probabilities))

        return entropy

    def calculate(self, outputs: pl.Series, target: pl.Series) -> tuple[float, float]:
        # TODO test this works with categorical values of eg string types
        mi = mutual_info_classif(outputs.to_numpy().reshape(-1, 1), target.to_numpy())[0]
        h_target = self._shannon_entropy(target.to_numpy())
        return (mi / h_target, self.uncertainty(len(outputs)))

class IncrementalEvaluator:
    def __init__(self, variant: FeatureVariant, inputs: FeatureInputs, metric: EfficientEvaluatorMetric, cost_per_second: float):
        self.variant = variant
        self.inputs = inputs
        self.metric = metric
        self.cost_per_second = cost_per_second

        # Feature outputs so far; taken from the first rows of the input, in order  
        self.outputs: pl.Series = pl.Series(values=[], dtype=variant.feature.dtype.polars_type) 
        self.metric_value = math.nan
        self.metric_uncertainty = 1.0 # Max uncertainty
        self.time_spent = datetime.timedelta(0)
        self.discarded_reason: str | None = None

    def effective_cost_per_row(self) -> float | None:
        if self.variant.cost_per_row:
            return self.variant.cost_per_row
        elif self.variant.time_per_row:
            return self.variant.time_per_row.total_seconds() * self.cost_per_second
        elif self.variant.min_rows_to_estimate_time_cost <= len(self.outputs) and self.time_spent > datetime.timedelta(0):
            return self.time_spent.total_seconds() * self.cost_per_second
        else:
            return None
        
    def effective_time_per_row(self) -> datetime.timedelta | None:
        if self.variant.time_per_row:
            return self.variant.time_per_row
        elif self.variant.cost_per_row:
            return datetime.timedelta(seconds=self.variant.cost_per_row / self.cost_per_second)
        elif self.time_spent > datetime.timedelta(0):
            return self.time_spent / len(self.outputs)
        else:
            return None
        
    def cost_spent(self) -> float:
        if len(self.outputs) == 0:
            return 0.0
        else:
            cost = self.effective_cost_per_row()
            if cost is None:
                return 0.0
            else:
                return cost * len(self.outputs)

    async def eval_next_batch(self, batch_size: int) -> None:
        """Batch-eval the next rows of the input, and update the metric and certainty."""
        batch_input = self.inputs.input.slice(self.evaluted_count(), batch_size)
        if len(batch_input) > 0:
            start_time = datetime.datetime.now()
            results = await self.variant.feature.aevaluate_batch(batch_input, self.inputs.conn)
            elapsed_time = datetime.datetime.now() - start_time
            self.outputs.append(results)
            self.time_spent += elapsed_time
            self.metric_value, self.metric_uncertainty = self.metric.calculate(self.outputs, self.inputs.target_column.head(len(self.outputs)))

    def evaluted_count(self) -> int:
        return len(self.outputs)

    def finished_evaluating(self) -> bool:
        return self.evaluted_count() == self.inputs.input.data.height

    async def finish_evaluating(self, default_batch_size: int) -> None:
        batch_size = self.variant.batch_size or default_batch_size
        while not self.finished_evaluating():
            await self.eval_next_batch(batch_size)

    def discard(self, reason: str) -> None:
        """Record a reason for state reporting"""
        self.discarded_reason = reason

    def state(self) -> FeatureVariantEvalState:
        return FeatureVariantEvalState(
            variant=self.variant,
            feature_outputs=self.outputs,
            metric_value=self.metric_value,
            metric_uncertainty=self.metric_uncertainty,
            time_spent=self.time_spent,
            cost_spent=self.cost_spent(),
            discarded_reason=self.discarded_reason,
            effective_cost_per_row=self.effective_cost_per_row(),
        )

@define(order=True)
class SortedIncrementalEvaluator:
   """Keep the evaluators sorted by cost, so we always try the cheapest first; if the cost is unknown, set it to zero, so we start by estimating those costs"""

   evaluator: IncrementalEvaluator = field(order=False)
   cost: float

   @staticmethod
   def wrap(evaluator: IncrementalEvaluator) -> SortedIncrementalEvaluator:
      return SortedIncrementalEvaluator(evaluator, evaluator.effective_cost_per_row() or 0.0)
   
@define
class SimpleEfficientEvaluator(EfficientEvaluator):
    def _potentially_valid(self, evaluator: IncrementalEvaluator, params: EfficientEvaluatorParams) -> bool:
        """Return False only if it definitely should not be considered anymore"""
        effective_cost, effective_time = evaluator.effective_cost_per_row(), evaluator.effective_time_per_row()
        if effective_cost is not None and params.max_cost is not None and effective_cost > params.max_cost:
            return False
        elif effective_time is not None and params.max_time is not None and effective_time > params.max_time:
            return False
        else:
            return True

    @override
    async def choose(self, 
                     variants: Sequence[FeatureVariant], inputs: FeatureInputs, metric: EfficientEvaluatorMetric,
                     params: EfficientEvaluatorParams, progress_callback: EfficientEvaluatorProgressCallback) -> EfficientEvaluatorResult:

        # TODO implement parallelization - evaluating several variants at once

        all_evaluators = tuple(SortedIncrementalEvaluator.wrap(IncrementalEvaluator(variant, inputs, metric, params.cost_per_second)) for variant in variants)

        await progress_callback.starting_states(tuple(e.evaluator.state() for e in all_evaluators))
        
        # Evaluators about whose metric we're not certain yet
        uncertain_evaluators = list(all_evaluators)
        heapq.heapify(uncertain_evaluators)
        
        # Remove variants that are known up front to have too high cost or time
        for sorted_evaluator in uncertain_evaluators:
            if not self._potentially_valid(sorted_evaluator.evaluator, params):
                _logger.debug(f'Discarding {sorted_evaluator.evaluator.variant}, it is too expensive')
                sorted_evaluator.evaluator.discard('Too expensive')
                await progress_callback.update(sorted_evaluator.evaluator.state())
        uncertain_evaluators = [e for e in uncertain_evaluators if self._potentially_valid(e.evaluator, params)]
        
        # Evaluators valid for being the chosen returned variant
        candidate_evaluators: list[SortedIncrementalEvaluator] = []

        def total_time_spent() -> datetime.timedelta:
            return sum((e.evaluator.time_spent for e in all_evaluators), start=datetime.timedelta(0))
        
        def out_of_evaluation_budget() -> bool:
            return params.max_cost_spent_evaluating is not None and sum(e.evaluator.cost_spent() for e in uncertain_evaluators) >= params.max_cost_spent_evaluating or \
                params.max_time_spent_evaluating is not None and total_time_spent() >= params.max_time_spent_evaluating
        
        # While we have the budget, keep evaluating variants until we're certain enough about their metrics,
        # starting with the cheapest variants. Variants with unknown cost are considered to be the cheapest 
        # until we find out their cost.
        while not out_of_evaluation_budget() and len(uncertain_evaluators) > 0:
            sorted_eval = heapq.heappop(uncertain_evaluators).evaluator 

            if not self._potentially_valid(sorted_eval, params):
                _logger.debug(f'Discarding {sorted_eval.variant}, it is too expensive')
                sorted_eval.discard('Too expensive')
                await progress_callback.update(sorted_eval.state())
            elif sorted_eval.metric_uncertainty > params.max_metric_uncertainty or sorted_eval.effective_cost_per_row() is None:
                if not sorted_eval.finished_evaluating():
                    batch_size = sorted_eval.variant.batch_size or params.default_batch_size
                    _logger.debug(f'Evaluating next {batch_size} rows with {sorted_eval.variant} to approximate its metric; '
                                  f'current approximate metric={sorted_eval.metric_value} with uncertainty={sorted_eval.metric_uncertainty}')
                    await sorted_eval.eval_next_batch(batch_size)
                    await progress_callback.update(sorted_eval.state())
                    heapq.heappush(uncertain_evaluators, SortedIncrementalEvaluator.wrap(sorted_eval))
                elif sorted_eval.metric_uncertainty > params.max_metric_uncertainty:
                    _logger.debug(f'{sorted_eval.variant} finished evaluating, but its metric is too uncertain, discarding: '
                                  f'metric={sorted_eval.metric_value}, uncertainty={sorted_eval.metric_uncertainty}')
                    sorted_eval.discard('Metric too uncertain after evaluating all data')
                    await progress_callback.update(sorted_eval.state())
                else:
                    _logger.info(f'{sorted_eval.variant} finished evaluating, but its cost is still unknown, discarding: '
                                 f'metric={sorted_eval.metric_value}, uncertainty={sorted_eval.metric_uncertainty}, evaluated {sorted_eval.evaluted_count()} rows, '
                                 f'min_rows_to_estimate_time_cost={sorted_eval.variant.min_rows_to_estimate_time_cost}')
                    sorted_eval.discard('Cost (time) unknown after evaluating all data')
                    await progress_callback.update(sorted_eval.state())
            elif sorted_eval.metric_value >= params.min_metric:
                _logger.debug(f'Candidate {sorted_eval.variant} has metric={sorted_eval.metric_value} with uncertainty={sorted_eval.metric_uncertainty}')
                candidate_evaluators.append(SortedIncrementalEvaluator.wrap(sorted_eval))
            else:
                _logger.debug(f'Discarding {sorted_eval.variant} with metric={sorted_eval.metric_value}, uncertainty={sorted_eval.metric_uncertainty}')
                sorted_eval.discard('Metric too low')
                await progress_callback.update(sorted_eval.state())


        # Discard candidates we didn't have enough budget to evaulate
        for evaluator in uncertain_evaluators:
            evaluator.evaluator.discard('Not enough budget to evaluate')
            await progress_callback.update(evaluator.evaluator.state())

        total_cost_spent = sum(e.evaluator.cost_spent() for e in all_evaluators)

        # Sanity check
        for candidate in candidate_evaluators:
            if candidate.evaluator.effective_cost_per_row() is None:
                raise ValueError('Effective cost for candidate is None')

        # Choose cheapest valid evaluator that has high enough certainty, if any, while obeying ROI
        if len(candidate_evaluators) == 0:
            _logger.debug('No candidate evaluators found')
            return EfficientEvaluatorResult(None, total_time_spent(), total_cost_spent)
        
        candidate = heapq.heappop(candidate_evaluators)
        while len(candidate_evaluators) > 0:
            next_candidate = heapq.heappop(candidate_evaluators)
            increase_in_metric = next_candidate.evaluator.metric_value - candidate.evaluator.metric_value
            increase_in_cost = next_candidate.cost - candidate.cost
            if increase_in_metric > 0 and increase_in_metric / increase_in_cost >= params.metric_per_cost_min_roi:
                candidate.evaluator.discard(f'Beaten on ROI: increase_in_metric={increase_in_metric}, increase_in_cost={increase_in_cost}, min_roi={params.metric_per_cost_min_roi}')
                await progress_callback.update(candidate.evaluator.state())
                candidate = next_candidate
            else:
                next_candidate.evaluator.discard('Insufficent ROI increase over current candidate')
                await progress_callback.update(next_candidate.evaluator.state())

        # Finish evaluating all rows to calculate the final stats
        if params.evaluate_chosen_variant_on_all_data:
            await candidate.evaluator.finish_evaluating(params.default_batch_size)
            await progress_callback.update(candidate.evaluator.state())

        return EfficientEvaluatorResult(candidate.evaluator.state(), total_time_spent(), total_cost_spent)
